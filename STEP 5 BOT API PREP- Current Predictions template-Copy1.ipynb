{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import scrape\n",
    "\n",
    "## marriage function \n",
    "\n",
    "path = 'all_stars/marriage/'\n",
    "mardir= os.listdir(path)\n",
    "pd.DataFrame(mardir)\n",
    "\n",
    "# select one of the married couples\n",
    "\n",
    "couple = mardir[0].replace('.csv','')\n",
    "couple\n",
    "\n",
    "# Extract Model Names\n",
    "\n",
    "long_model,short_model = couple.split('_')[1],couple.split('_')[3]\n",
    "print('LONG:',long_model,'\\nSHORT:',short_model)\n",
    "\n",
    "#'''HERE'''\n",
    "#DO A IF MODEL PATH NOT REAL THEN \n",
    "#>MOVER.MOVE_MODEL\n",
    "# ALSO MAKE IT OVERWRITE A TXT FILE EVERYTIME WITH FEATURES\n",
    "\n",
    "import mover\n",
    "model,info,bt,was_long,was_scaled,up_mpl,dn_mpl = mover.load_model(long_model)\n",
    "\n",
    "# [ data inputs ]\n",
    "\n",
    "# this may change when you start moving the thing aroung\n",
    "apath = 'all_stars/'\n",
    "\n",
    "#params for urllib : get_some\n",
    "ticker = 'DMTK'\n",
    "url_param = 'historical-chart/1hour'\n",
    "\n",
    "\n",
    "new_data_name = ticker + '_'+url_param.replace('/','_')+'.csv'\n",
    "\n",
    "# {BEGIN DATA FUNCTIONS}\n",
    "\n",
    "\n",
    "\n",
    "#find the unscaled training data\n",
    "datapath=apath+'clean_data/'\n",
    "dapali = os.listdir(datapath)\n",
    "data   = [i for i in dapali if 'features' in i][0]\n",
    "dfpath = datapath + data\n",
    "old_df     = pd.read_csv(dfpath)#.set_index('Unnamed: 0')\n",
    "old_df.tail()\n",
    "\n",
    "## Pull Recent Data\n",
    "\n",
    "\n",
    "#downloads recent data TAKES - url and ticker\n",
    "price_data = scrape.get_some(url_param,ticker)\n",
    "price_data\n",
    "\n",
    "#load data from csv so its free of bullshit\n",
    "dnld_path = 'downloaded_data/'\n",
    "price_path = dnld_path+new_data_name\n",
    "df = pd.read_csv(price_path,index_col='date')\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df = df.drop('Unnamed: 0',axis=1)\n",
    "    \n",
    "df.index = pd.to_datetime(df.index)\n",
    "df['date'] = df.index.date\n",
    "df = df[::-1]\n",
    "\n",
    "df\n",
    "\n",
    "# {these are the features}\n",
    "\n",
    "'''\n",
    "in the future i want the original feature creatation to\n",
    "be called by a script with a standard name \n",
    "and then you can just call it here and ya done!\n",
    "'''\n",
    "\n",
    "#import candle_sticks\n",
    "#import daily_close_compare\n",
    "\n",
    "# Technical aylisis features\n",
    "\n",
    "import ta\n",
    "\n",
    "df = ta.add_all_ta_features(df,'open','high','low','close','volume',fillna=True)\n",
    "\n",
    "# Candels features\n",
    "\n",
    "df.index\n",
    "\n",
    "\n",
    "# adds all the candel patterns\n",
    "#candle_sticks.all_candels(df)\n",
    "\n",
    "#adds the daily_close_comparison class i made [which the day checker is not working btw]\n",
    "#df = daily_close_compare.create_anna(df)\n",
    "df = df.drop('date',axis=1)\n",
    "\n",
    "def jugjug_fucking_bowdown(df):\n",
    "    for col in df.columns:\n",
    "        df = df.rename(columns={col:col.lower()})\n",
    "    return df\n",
    "\n",
    "#make all columns lower case\n",
    "df = jugjug_fucking_bowdown(df)\n",
    "old_df= jugjug_fucking_bowdown(old_df)\n",
    "df\n",
    "\n",
    "if 'time' in old_df.columns:\n",
    "    old_df = old_df.set_index('time')\n",
    "old_df\n",
    "\n",
    "# cheack lens of columns\n",
    "\n",
    "print(len(df.columns),len(old_df.columns))\n",
    "\n",
    "# this will get em in order\n",
    "df = df[old_df.columns]\n",
    "\n",
    "## mix em together\n",
    "\n",
    "# make the indice datetime\n",
    "df.index     = pd.to_datetime(df.index)\n",
    "old_df.index = pd.to_datetime(old_df.index)\n",
    "\n",
    "#last value on training data\n",
    "last_value   = old_df.index[-1]\n",
    "#create mask\n",
    "df           = df[df.index>last_value]\n",
    "\n",
    "#mix_old and new\n",
    "df = old_df.append(df)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.heatmap(df.isnull())\n",
    "\n",
    "df\n",
    "\n",
    "# {BEGIN MODEL FUNCTIONS}\n",
    "\n",
    "## [inputs]\n",
    "\n",
    "model_name = long_model\n",
    "import mover #.................................................................................\n",
    "model,info,bt,was_long,was_scaled,up_mpl,dn_mpl = mover.load_model(long_model)\n",
    "\n",
    "\n",
    "## scale \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale(df):\n",
    "    scale = StandardScaler()\n",
    "    scaled= scale.fit_transform(df)\n",
    "    sdf   = pd.DataFrame(scaled,columns=df.columns)\n",
    "    sdf.index = df.index\n",
    "    return sdf\n",
    "\n",
    "\n",
    "def get_pred(df,was_long,was_scaled):\n",
    "    #scale input data if it was scaled\n",
    "    if was_scaled:\n",
    "        input_data = scale(df) \n",
    "        print('scaled')\n",
    "    else:\n",
    "        input_data = df.copy()\n",
    "        print('not scaled')\n",
    "    #name it long if it was long\n",
    "    if was_long:\n",
    "        name = 'long_'+model_name\n",
    "    else:\n",
    "        name = 'short_'+model_name\n",
    "\n",
    "    # predictions\n",
    "\n",
    "    pred = model.predict(input_data)\n",
    "    pred\n",
    "\n",
    "    name\n",
    "\n",
    "    ## make a prediction dataFrame\n",
    "\n",
    "    pdf = df[['open','close','high','low']]\n",
    "    pdf[name] = pred\n",
    "    pdf[name] = pdf[name].replace(True,1).replace(1,pdf.close)\n",
    "\n",
    "    return [name,pred,pdf]\n",
    "\n",
    "\n",
    "\n",
    "model_name = long_model\n",
    "import mover #.................................................................................\n",
    "model,info,bt,was_long,was_scaled,up_mpl,dn_mpl = mover.load_model(model_name)\n",
    "\n",
    "\n",
    "model,info,bt,was_long,was_scaled,up_mpl,dn_mpl = mover.load_model(model_name)\n",
    "name,pred,pdf = get_pred(df,was_long,was_scaled)\n",
    "\n",
    "pdf\n",
    "\n",
    "short_model\n",
    "\n",
    "model_name = short_model\n",
    "\n",
    "model,info,bt,was_long,was_scaled,up_mpl,dn_mpl = mover.load_model(model_name)\n",
    "sname,spred,spdf = get_pred(df,was_long,was_scaled)\n",
    "\n",
    "## mix scaled\n",
    "\n",
    "pdf[sname] = spdf[sname]\n",
    "pdf\n",
    "\n",
    "import cufflinks as cf\n",
    "cf.go_offline(connected=False)\n",
    "def sola(df):\n",
    "    return df.iplot(theme='solar',fill=True)\n",
    "\n",
    "\n",
    "sola(pdf[[sname,'close',name]])\n",
    "\n",
    "\n",
    "#highlight de boo-LEANS!\n",
    "def hl(df):\n",
    "    def highlight(boo):\n",
    "        criteria = boo == True\n",
    "        return['background-color: green'if i else '' for i in criteria]\n",
    "    df = df.style.apply(highlight)\n",
    "    return df\n",
    "\n",
    "#prediction df for highlighted booleans\n",
    "predf = df[['open','close','low','high']]\n",
    "predf[name] = pred\n",
    "predf[sname]= spred\n",
    "#plot most recent data on top\n",
    "hl(predf[::-1])\n",
    "\n",
    "## on off plot and add to predf.. then ya done\n",
    "\n",
    "# this creates a bionary plot thats on when long model says long\n",
    "# until short model says short...\n",
    "predf = predf.reset_index()\n",
    "predf['bionary'] = False\n",
    "for i in range(1,len(predf)):\n",
    "    #name is long\n",
    "    if predf[name][i] >0:\n",
    "        predf['bionary'][i] = True\n",
    "    #sname is short\n",
    "    elif predf[sname][i] >0:\n",
    "        predf['bionary'][i] = False\n",
    "    else:\n",
    "        predf['bionary'][i] = predf['bionary'][i-1]\n",
    "\n",
    "predf = predf.set_index('index')\n",
    "\n",
    "# create one that does the oposite\n",
    "predf['short_bionary'] = predf['bionary']==False\n",
    "\n",
    "#scale em\n",
    "predf['bionary']       = predf['bionary'].replace(True,1).replace(1,predf.close)\n",
    "predf['short_bionary'] = predf['short_bionary'].replace(True,1).replace(1,predf.close)\n",
    "# plot em\n",
    "bionary_predf = predf[['short_bionary','close','bionary']]\n",
    "\n",
    "\n",
    "sola(bionary_predf)\n",
    "\n",
    "ouput = [bionary_predf,pdf,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once you get it worked out here \n",
    "### 1. copy this\n",
    "### 2. merge this\n",
    "### 3. put in stepfive.py\n",
    "### BAM! you got a bot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "### add the favorite target std_bands at the end of this"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
